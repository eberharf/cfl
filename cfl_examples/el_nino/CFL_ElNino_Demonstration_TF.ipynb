{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iman Wahle\n",
    "# August 2020\n",
    "# Adapted from CFL_ElNino_Demonstration.ipynb by Krzysztof Chalupka\n",
    "# This notebook walks through aggregating microlevel climate data to\n",
    "# macrolevel features as causal hypotheses for future testing. The neural\n",
    "# network used for density learning has been adapted from the original\n",
    "# implementation to use tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import core_ml_tf as cmt # new tf backend\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imshape = (55, 9) # This is the grid shape of our images, stored here for plotting reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset\n",
    "-----------\n",
    "\n",
    "The following code loads our data into the correct format:\n",
    "* X -- numpy array of size (n_datapoints, n_input_dim). Each row corresponds to one input value.\n",
    "* Y -- numpy array of size (n_datapoints, n_output_dim). Each row is one output value.\n",
    "\n",
    "In our case, each row of X is a (flattened) map of Pacific zonal wind strength, and each row of Y a (flattened) map of Pacific water temperature over the same region. In our case, n_input_dim == n_output_dim, but this need not be the case at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imanwahle/anaconda2/envs/jntfpy3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: The file 'elnino_data.pkl' has been generated with a joblib version less than 0.10. Please regenerate this pickle file.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import joblib \n",
    "\n",
    "# Load the data. \n",
    "## PLUG YOUR OWN DATA HERE. 'coords' is only needed to display climate maps. Your data\n",
    "## can only contain X and Y arrays.\n",
    "X, Y, coords = joblib.load('elnino_data.pkl')\n",
    "\n",
    "# Create a randomized, normalized training and validation set.\n",
    "x_scaler = StandardScaler().fit(X)\n",
    "y_scaler = StandardScaler().fit(Y)\n",
    "X_tr, X_ts, Y_tr, Y_ts = train_test_split(X, Y, shuffle=True, train_size=0.85)\n",
    "X_tr = x_scaler.transform(X_tr).astype('float32')\n",
    "Y_tr = y_scaler.transform(Y_tr).astype('float32')\n",
    "X_ts = x_scaler.transform(X_ts).astype('float32')\n",
    "Y_ts = y_scaler.transform(Y_ts).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (13140, 495)\n",
      "Y.shape: (13140, 495)\n"
     ]
    }
   ],
   "source": [
    "print('X shape: {}'.format(X.shape))\n",
    "print('Y.shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning P(Y | X)\n",
    "--------------------\n",
    "The first step of Causal Feature Learning (CFL) is to cluster x's according to P(Y | x) conditional densities. In this demonstration, for simplicity, we will approximate P(Y | x) with its expected value E[P(Y | x)]. This means we assume that if two distributions have equal means, they themselves are equal. It is possible to efficiently relax this assumption by using Mixture Density Networks (Bishop 1995) to approximate all moments of a distribution.\n",
    "\n",
    "Learning E[P(Y | x)] amounts to regressing y on x. We do this using a neural network and the Lasagne package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "nn_input_layer (InputLayer)  [(None, 495)]             0         \n",
      "_________________________________________________________________\n",
      "nn_dropout1 (Dropout)        (None, 495)               0         \n",
      "_________________________________________________________________\n",
      "nn_dense1 (Dense)            (None, 1024)              507904    \n",
      "_________________________________________________________________\n",
      "nn_dropout2 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "nn_layer2 (Dense)            (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "nn_dropout3 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "nn_output_layer (Dense)      (None, 495)               507375    \n",
      "=================================================================\n",
      "Total params: 2,064,879\n",
      "Trainable params: 2,064,879\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test every 300 epochs\n",
      "Epoch 0/3001: train_loss: 8.211238861083984, test_loss: 0.5159462094306946\n",
      "Saving weights to  checkpoints/epoch_0\n",
      "Epoch 300/3001: train_loss: 0.49892139434814453, test_loss: 0.4449182450771332\n",
      "Saving weights to  checkpoints/epoch_300\n"
     ]
    }
   ],
   "source": [
    "cmt.train_network(X_tr, Y_tr, X_ts, Y_ts, \n",
    "                  save_fname='checkpoints/epoch_{}', \n",
    "                  n_epochs=3001,\n",
    "                  lr=1e-3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cmt.get_model(X.shape[1], Y.shape[1])\n",
    "model.load_weights('checkpoints/epoch_2400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finding the Observational Partition of X\n",
    "-----------------------------------------\n",
    "Finding the observational partition of X amounts to clustering the X data according to P(Y | X). That is, we put two x's in the same bucket if the neural net we trained maps them to the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 4\n",
    "yhat = model.predict(X)\n",
    "x_lbls = KMeans(n_clusters=N_CLASSES, n_init=10, n_jobs=-1).fit_predict(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Observational Partition of Y\n",
    "---------------------------------------------\n",
    "To find the partition of Y, we want to put together all y1 and y2 if P(y1 | x) == P(y2 | x) for each x. This procedure is a little bit more interesting than clustering the x's, and is described in our UAI 2016 paper. We describe it briefly here.\n",
    "\n",
    "We've already clustered x's in a way that that guarantees that if x1 and x2 belong to the same x_lbls class, then P(y | x1) == P(y | x2). Thus, the requirement P(y1 | x) == P(y2 | x) can be subsituted by P(y1 | x_lbls==O) == P(y2 | x_lbls==O) for any observational X-class O. Since x_lbls is discrete, we should have plenty of data per each x_lbls class. We will approximate P(y | x_lbls==O) using the distance of y to the closest (except for itself) Y-point whose corresponding x belongs to x_lbls==O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ftrs = np.zeros((Y.shape[0], np.unique(x_lbls).size))\n",
    "# Loop, not vectorized, to save memory. Can take a while.\n",
    "for y_id, y in enumerate(np.vstack([Y_tr, Y_ts])):\n",
    "    if y_id % 100==0:\n",
    "        sys.stdout.write('\\rComputing P(y | x_lbls) features, iter {}/{}...'.format(y_id, Y.shape[0]))\n",
    "        sys.stdout.flush() \n",
    "    for x_lbl_id, x_lbl in enumerate(np.unique(x_lbls)):\n",
    "        # Find ids of xs in this x_lbls class.\n",
    "        x_lbl_ids = np.where(x_lbls==x_lbl)[0]\n",
    "        # Compute distances of y to all y's in this x_lbls class and sort them.\n",
    "        sorted_dists = np.sort(np.sum((y-np.vstack([Y_tr, Y_ts])[x_lbl_ids])**2, axis=1))\n",
    "        # Find the mean distance to the 4 closest points (exclude the actually closest point though).\n",
    "        y_ftrs[y_id][x_lbl_id] = sorted_dists[1:5].mean()\n",
    "print('Done. Clustering P(y | x_lbls).')\n",
    "y_lbls = KMeans(n_clusters=N_CLASSES, n_init=10, n_jobs=-1).fit_predict(y_ftrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Results\n",
    "--------------------------\n",
    "Visualizing the observational partition is data-specific. In our case, since both X and Y are images, we can visualize the means of each observational cluster to gain some insight into what it contains. \n",
    "\n",
    "But, one of the virtues of the method is that it is interpretation-agnostic. The observational partition can be used as a causal hypothesis to drive experimentation. This can be done whether the inputs and outputs are easily interpretable or not at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10), facecolor='white')\n",
    "X_raw = x_scaler.inverse_transform(np.vstack([X_tr, X_ts]))\n",
    "Y_raw = y_scaler.inverse_transform(np.vstack([Y_tr, Y_ts]))\n",
    "\n",
    "levels = np.linspace(-0.5,0.5,30)\n",
    "for x_cluster_id in range(4):\n",
    "    ax = plt.subplot2grid((4,2), (x_cluster_id, 0))  \n",
    "    # Plot the cluster's mean difference from all frames' mean.\n",
    "    cluster_mean = (X_raw[x_lbls==x_cluster_id].mean(axis=0)-X_raw.mean(axis=0)).reshape(imshape).T\n",
    "    im=ax.contourf(coords['x'].ravel(), coords['y'].ravel(), cluster_mean, levels=levels, cmap='BrBG_r')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "levels=np.linspace(-1,1.5,30)\n",
    "for y_cluster_id in range(4):\n",
    "    ax = plt.subplot2grid((4,2), (y_cluster_id, 1))  \n",
    "    # Plot the cluster's mean difference from all frames' mean.\n",
    "    cluster_mean = (Y_raw[y_lbls==y_cluster_id].mean(axis=0)-Y_raw.mean(axis=0)).reshape(imshape).T\n",
    "\n",
    "    im=ax.contourf(coords['x'].ravel(), coords['y'].ravel(), cluster_mean, levels=levels, cmap='coolwarm')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "# Compute and print P(y_lbl | x_lbl)\n",
    "P_CE = np.array([np.bincount(y_lbls.astype(int)[x_lbls==x_lbl], \n",
    "    minlength=y_lbls.max()+1).astype(float) for x_lbl in np.sort(np.unique(x_lbls))])\n",
    "P_CE = P_CE/P_CE.sum()\n",
    "P_E_given_C = P_CE/P_CE.sum(axis=1, keepdims=True)\n",
    "\n",
    "print('P(TempCluster | WindCluster):')\n",
    "print(P_E_given_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jntfpy3",
   "language": "python",
   "name": "jntfpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
