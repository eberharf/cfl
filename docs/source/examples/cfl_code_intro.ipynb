{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFL Code Demo and Explanation: Visual Bars \n",
    "This example demonstrates how to run a basic CFL experiment on Visual Bars Data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points for Loading Data\n",
    "1. The data should consist of a data set `X` and a data set `Y` that are aligned (each row in `X` corresponds to the row with the same index in `Y`)\n",
    "2. `X` should be the causal data set and `Y` should be the effect data \n",
    "3. For most instances of CFL, `X` and `Y` should be reshaped such that each one is a 2D array with dimensions (n_samples, n_features). Some instances of CFL require that `X` be a 4-D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Visual Bars Data\n",
    "To create a visual bars data set, we need to import the file `generate_visual_bars_data.py`. If you are trying to generate the visual bars data from outside of the root directory of the `cfl` reposity, add the `visual_bars` directory path to the PYTHONPATH (same as you did for the `cfl` package) for easy importing. \n",
    "\n",
    "See the [visual bars page](https://cfl.readthedocs.io/en/latest/more_info/Visual_Bars_data.html) for background on the visual bars data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the file to generate visual bars data \n",
    "import visual_bars.generate_visual_bars_data as vbd\n",
    "\n",
    "# uncomment this line and use it instead if you have added `visual_bars` to the pythonpath\n",
    "# import generate_visual_bars_data as vbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to generate visual bars data, we set the number of images we want to generate \n",
    "# (`n_samples`), the size of each image in pixels (`im_shape`), and the intensity of random noise\n",
    "# in the image (`noise_lvl`). To have reproducible results, we also will set a random seed. \n",
    "\n",
    "# create visual bars data \n",
    "n_samples = 5000 \n",
    "im_shape = (10, 10) \n",
    "noise_lvl= 0.03\n",
    "random_state = 180\n",
    "\n",
    "vb_data = vbd.VisualBarsData(n_samples=n_samples, im_shape=im_shape, \n",
    "                             noise_lvl=noise_lvl, set_random_seed=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10, 10)\n",
      "(5000,)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# We save an array of images to a variable `X` and the array of target behavior to a variable \n",
    "# `Y`. Note that `X` and `Y` are aligned - they must have the same number of observations, and \n",
    "# the nth image in `X` must correspond to the nth target value in `Y`. \n",
    "\n",
    "# retrieve the images and the target \n",
    "X = vb_data.getImages()\n",
    "Y = vb_data.getTarget()\n",
    "\n",
    "# X and Y have the same number of rows  \n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X.shape[0]==Y.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shaping the Data \n",
    "Because our `X` dataset is a set of images, we will use `CondExpCNN` for our CDE. This CDE requires that `X` be a 4-D array with dimensions `(n_samples, im_height, im_width, n_channels)` and `Y` to be 2-dimensional, with shape `(n_samples, n_features)`. \n",
    "\n",
    "The 'basic' CDE `CondExpMod`, which is better suited to non-spatially-organized data, requires a different input shape than `CondExpCNN`. For further guidelines about shaping your data, see [info about CDEs](https://cfl.readthedocs.io/en/latest/CDEs.html#input-shape-for-cdes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10, 10, 1)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "#reformat x, y into the right shape for the neural net \n",
    "\n",
    "# expand X \n",
    "X = np.expand_dims(X, -1) \n",
    "print(X.shape) #black and white images have just one channel\n",
    "\n",
    "# expand Y\n",
    "Y = np.expand_dims(Y, -1)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up the CFL Pipeline \n",
    "\n",
    "We will now set up an `Experiment`. The `Experiment` will create a CFL pipeline and automatically save all the parameters, results, and the trained models generated during this experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFL takes in 3 sets of parameters, each in dictionary form: \n",
    "\n",
    "- `data_info`  \n",
    "- `CDE_params`   \n",
    "- `cluster_params`   \n",
    "\n",
    "For further details on the meaning of these parameters, consult the documentation for the clusterer and the CondExp base class. \n",
    "\n",
    "In this case, we use a convolutional neural net for the CDE, and K-means for clustering. Consult the documentation for the other available models. \n",
    "\n",
    "Note that we didn't specify some parameters, and so those parameters are given default values (values printed below). \n",
    "\n",
    "Here we only are interested in clustering our cause space. For an example\n",
    "of how to cluster both cause and effect spaces, please refer to the El Ni√±o tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results from this run will be saved to visual_bars_cfl/experiment0010\n",
      "verbose not specified in input, defaulting to 1\n",
      "weights_path not specified in input, defaulting to None\n",
      "show_plot not specified in input, defaulting to True\n",
      "standardize not specified in input, defaulting to False\n",
      "tb_path not specified in input, defaulting to None\n",
      "optuna_callback not specified in input, defaulting to None\n",
      "optuna_trial not specified in input, defaulting to None\n",
      "early_stopping not specified in input, defaulting to False\n",
      "checkpoint_name not specified in input, defaulting to tmp_checkpoints\n",
      "tune not specified in input, defaulting to False\n",
      "verbose not specified in input, defaulting to 1\n"
     ]
    }
   ],
   "source": [
    "from cfl.experiment import Experiment\n",
    "\n",
    "# the parameters should be passed in dictionary form \n",
    "data_info = {'X_dims' : X.shape, \n",
    "             'Y_dims' : Y.shape, \n",
    "             'Y_type' : 'categorical' #options: 'categorical' or 'continuous'\n",
    "            }\n",
    "\n",
    "# pass in empty parameter dictionaries to use the default parameter values (not \n",
    "# allowed for data_info)\n",
    "cnn_params = {  'model'            : 'CondExpCNN',\n",
    "                'model_params'     : {\n",
    "                    'filters'          : [8],\n",
    "                    'input_shape'      : data_info['X_dims'][1:],\n",
    "                    'kernel_size'      : [(4, 4)],\n",
    "                    'pool_size'        : [(2, 2)],\n",
    "                    'padding'          : ['same'],\n",
    "                    'conv_activation'  : ['relu'],\n",
    "                    'dense_units'      : 16,\n",
    "                    'dense_activation' : 'relu',\n",
    "                    'output_activation': None,\n",
    "\n",
    "                    'batch_size'  : 84, # parameters for training \n",
    "                    'n_epochs'    : 500,\n",
    "                    'optimizer'   : 'adam',\n",
    "                    'opt_config'  : {'lr' : 1e-5},\n",
    "                    'loss'        : 'mean_squared_error',\n",
    "                    'best'        : True, \n",
    "                    'verbose'     : 1\n",
    "                }\n",
    "            }\n",
    "\n",
    "cause_cluster_params = {'model' : 'KMeans', \n",
    "                        'model_params' : {\n",
    "                            'n_clusters' : 4, \n",
    "                            'verbose'    : 1\n",
    "                        }\n",
    "} \n",
    "\n",
    "# steps of this CFL pipeline\n",
    "block_names = ['CondDensityEstimator', 'CauseClusterer']\n",
    "block_params = [cnn_params, cause_cluster_params]\n",
    "\n",
    "# folder to save results to \n",
    "save_path = 'visual_bars_cfl' \n",
    "\n",
    "# create the experiment!\n",
    "my_exp = Experiment(X_train=X, Y_train=Y, data_info=data_info, block_names=block_names, block_params=block_params, results_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that any parameters not specified will be given default values (shown in the print statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with a CFL object  \n",
    "\n",
    "We now train the CFL object on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Beginning CFL Experiment training. ####################\n",
      "Beginning CondDensityEstimator training...\n",
      "No GPU device detected.\n",
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/500\n",
      "3750/3750 [==============================] - 15s 4ms/sample - loss: 0.2590 - val_loss: 0.2545\n",
      "Epoch 2/500\n",
      "3750/3750 [==============================] - 2s 470us/sample - loss: 0.2470 - val_loss: 0.2430\n",
      "Epoch 3/500\n",
      "3750/3750 [==============================] - 2s 472us/sample - loss: 0.2368 - val_loss: 0.2332\n",
      "Epoch 4/500\n",
      "3750/3750 [==============================] - 2s 515us/sample - loss: 0.2283 - val_loss: 0.2252\n",
      "Epoch 5/500\n",
      "3750/3750 [==============================] - 2s 448us/sample - loss: 0.2212 - val_loss: 0.2186\n",
      "Epoch 6/500\n",
      "3750/3750 [==============================] - 1s 398us/sample - loss: 0.2154 - val_loss: 0.2132\n",
      "Epoch 7/500\n",
      "3750/3750 [==============================] - 2s 425us/sample - loss: 0.2107 - val_loss: 0.2087\n",
      "Epoch 8/500\n",
      "3750/3750 [==============================] - 2s 454us/sample - loss: 0.2069 - val_loss: 0.2053\n",
      "Epoch 9/500\n",
      "3750/3750 [==============================] - 2s 448us/sample - loss: 0.2039 - val_loss: 0.2024\n",
      "Epoch 10/500\n",
      "3750/3750 [==============================] - 3s 866us/sample - loss: 0.2014 - val_loss: 0.2001\n",
      "Epoch 11/500\n",
      "3750/3750 [==============================] - 2s 523us/sample - loss: 0.1993 - val_loss: 0.1981\n",
      "Epoch 12/500\n",
      "3750/3750 [==============================] - 2s 464us/sample - loss: 0.1975 - val_loss: 0.1965\n",
      "Epoch 13/500\n",
      "3750/3750 [==============================] - 2s 526us/sample - loss: 0.1960 - val_loss: 0.1951\n",
      "Epoch 14/500\n",
      "3750/3750 [==============================] - 2s 472us/sample - loss: 0.1945 - val_loss: 0.1937\n",
      "Epoch 15/500\n",
      "3750/3750 [==============================] - 2s 446us/sample - loss: 0.1933 - val_loss: 0.1926\n",
      "Epoch 16/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1921 - val_loss: 0.1914\n",
      "Epoch 17/500\n",
      "3750/3750 [==============================] - 2s 477us/sample - loss: 0.1910 - val_loss: 0.1904\n",
      "Epoch 18/500\n",
      "3750/3750 [==============================] - 2s 565us/sample - loss: 0.1899 - val_loss: 0.1895\n",
      "Epoch 19/500\n",
      "3750/3750 [==============================] - 2s 451us/sample - loss: 0.1889 - val_loss: 0.1885\n",
      "Epoch 20/500\n",
      "3750/3750 [==============================] - 2s 542us/sample - loss: 0.1879 - val_loss: 0.1876\n",
      "Epoch 21/500\n",
      "3750/3750 [==============================] - 2s 604us/sample - loss: 0.1870 - val_loss: 0.1867\n",
      "Epoch 22/500\n",
      "3750/3750 [==============================] - 3s 690us/sample - loss: 0.1861 - val_loss: 0.1859\n",
      "Epoch 23/500\n",
      "3750/3750 [==============================] - 2s 604us/sample - loss: 0.1853 - val_loss: 0.1851\n",
      "Epoch 24/500\n",
      "3750/3750 [==============================] - 3s 690us/sample - loss: 0.1844 - val_loss: 0.1843\n",
      "Epoch 25/500\n",
      "3750/3750 [==============================] - 2s 575us/sample - loss: 0.1836 - val_loss: 0.1836\n",
      "Epoch 26/500\n",
      "3750/3750 [==============================] - 3s 821us/sample - loss: 0.1829 - val_loss: 0.1829\n",
      "Epoch 27/500\n",
      "3750/3750 [==============================] - 2s 562us/sample - loss: 0.1821 - val_loss: 0.1822\n",
      "Epoch 28/500\n",
      "3750/3750 [==============================] - 3s 751us/sample - loss: 0.1814 - val_loss: 0.1815\n",
      "Epoch 29/500\n",
      "3750/3750 [==============================] - 2s 438us/sample - loss: 0.1807 - val_loss: 0.1808\n",
      "Epoch 30/500\n",
      "3750/3750 [==============================] - 3s 675us/sample - loss: 0.1800 - val_loss: 0.1802\n",
      "Epoch 31/500\n",
      "3750/3750 [==============================] - 2s 498us/sample - loss: 0.1794 - val_loss: 0.1796\n",
      "Epoch 32/500\n",
      "3750/3750 [==============================] - 2s 439us/sample - loss: 0.1788 - val_loss: 0.1790\n",
      "Epoch 33/500\n",
      "3750/3750 [==============================] - 2s 468us/sample - loss: 0.1781 - val_loss: 0.1785\n",
      "Epoch 34/500\n",
      "3750/3750 [==============================] - 2s 510us/sample - loss: 0.1776 - val_loss: 0.1779\n",
      "Epoch 35/500\n",
      "3750/3750 [==============================] - 2s 460us/sample - loss: 0.1770 - val_loss: 0.1773\n",
      "Epoch 36/500\n",
      "3750/3750 [==============================] - 2s 445us/sample - loss: 0.1764 - val_loss: 0.1768\n",
      "Epoch 37/500\n",
      "3750/3750 [==============================] - 2s 583us/sample - loss: 0.1758 - val_loss: 0.1763\n",
      "Epoch 38/500\n",
      "3750/3750 [==============================] - 2s 555us/sample - loss: 0.1753 - val_loss: 0.1758\n",
      "Epoch 39/500\n",
      "3750/3750 [==============================] - 3s 761us/sample - loss: 0.1748 - val_loss: 0.1753\n",
      "Epoch 40/500\n",
      "3750/3750 [==============================] - 3s 677us/sample - loss: 0.1743 - val_loss: 0.1748\n",
      "Epoch 41/500\n",
      "3750/3750 [==============================] - 2s 425us/sample - loss: 0.1738 - val_loss: 0.1743\n",
      "Epoch 42/500\n",
      "3750/3750 [==============================] - 2s 442us/sample - loss: 0.1733 - val_loss: 0.1739\n",
      "Epoch 43/500\n",
      "3750/3750 [==============================] - 2s 457us/sample - loss: 0.1728 - val_loss: 0.1734\n",
      "Epoch 44/500\n",
      "3750/3750 [==============================] - 2s 542us/sample - loss: 0.1723 - val_loss: 0.1730\n",
      "Epoch 45/500\n",
      "3750/3750 [==============================] - 2s 465us/sample - loss: 0.1718 - val_loss: 0.1725\n",
      "Epoch 46/500\n",
      "3750/3750 [==============================] - 2s 444us/sample - loss: 0.1714 - val_loss: 0.1721\n",
      "Epoch 47/500\n",
      "3750/3750 [==============================] - 2s 427us/sample - loss: 0.1709 - val_loss: 0.1716\n",
      "Epoch 48/500\n",
      "3750/3750 [==============================] - 2s 470us/sample - loss: 0.1705 - val_loss: 0.1712\n",
      "Epoch 49/500\n",
      "3750/3750 [==============================] - 2s 562us/sample - loss: 0.1701 - val_loss: 0.1708\n",
      "Epoch 50/500\n",
      "3750/3750 [==============================] - 2s 471us/sample - loss: 0.1696 - val_loss: 0.1704\n",
      "Epoch 51/500\n",
      "3750/3750 [==============================] - 2s 560us/sample - loss: 0.1692 - val_loss: 0.1699\n",
      "Epoch 52/500\n",
      "3750/3750 [==============================] - 2s 456us/sample - loss: 0.1688 - val_loss: 0.1696\n",
      "Epoch 53/500\n",
      "3750/3750 [==============================] - 3s 923us/sample - loss: 0.1684 - val_loss: 0.1692\n",
      "Epoch 54/500\n",
      "3750/3750 [==============================] - 2s 629us/sample - loss: 0.1680 - val_loss: 0.1687\n",
      "Epoch 55/500\n",
      "3750/3750 [==============================] - 2s 544us/sample - loss: 0.1676 - val_loss: 0.1684\n",
      "Epoch 56/500\n",
      "3750/3750 [==============================] - 2s 574us/sample - loss: 0.1672 - val_loss: 0.1680\n",
      "Epoch 57/500\n",
      "3750/3750 [==============================] - 2s 452us/sample - loss: 0.1668 - val_loss: 0.1676\n",
      "Epoch 58/500\n",
      "3750/3750 [==============================] - 2s 467us/sample - loss: 0.1665 - val_loss: 0.1672\n",
      "Epoch 59/500\n",
      "3750/3750 [==============================] - 2s 499us/sample - loss: 0.1661 - val_loss: 0.1669\n",
      "Epoch 60/500\n",
      "3750/3750 [==============================] - 2s 449us/sample - loss: 0.1657 - val_loss: 0.1665\n",
      "Epoch 61/500\n",
      "3750/3750 [==============================] - 2s 480us/sample - loss: 0.1654 - val_loss: 0.1661\n",
      "Epoch 62/500\n",
      "3750/3750 [==============================] - 2s 451us/sample - loss: 0.1650 - val_loss: 0.1658\n",
      "Epoch 63/500\n",
      "3750/3750 [==============================] - 2s 507us/sample - loss: 0.1647 - val_loss: 0.1654\n",
      "Epoch 64/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1643 - val_loss: 0.1651\n",
      "Epoch 65/500\n",
      "3750/3750 [==============================] - 2s 489us/sample - loss: 0.1640 - val_loss: 0.1648\n",
      "Epoch 66/500\n",
      "3750/3750 [==============================] - 2s 555us/sample - loss: 0.1637 - val_loss: 0.1644\n",
      "Epoch 67/500\n",
      "3750/3750 [==============================] - 2s 458us/sample - loss: 0.1633 - val_loss: 0.1641\n",
      "Epoch 68/500\n",
      "3750/3750 [==============================] - 2s 459us/sample - loss: 0.1630 - val_loss: 0.1638\n",
      "Epoch 69/500\n",
      "3750/3750 [==============================] - 2s 463us/sample - loss: 0.1627 - val_loss: 0.1635\n",
      "Epoch 70/500\n",
      "3750/3750 [==============================] - 2s 600us/sample - loss: 0.1624 - val_loss: 0.1631\n",
      "Epoch 71/500\n",
      "3750/3750 [==============================] - 2s 447us/sample - loss: 0.1621 - val_loss: 0.1628\n",
      "Epoch 72/500\n",
      "3750/3750 [==============================] - 2s 411us/sample - loss: 0.1617 - val_loss: 0.1625\n",
      "Epoch 73/500\n",
      "3750/3750 [==============================] - 2s 420us/sample - loss: 0.1615 - val_loss: 0.1622\n",
      "Epoch 74/500\n",
      "3750/3750 [==============================] - 2s 501us/sample - loss: 0.1612 - val_loss: 0.1619\n",
      "Epoch 75/500\n",
      "3750/3750 [==============================] - 2s 437us/sample - loss: 0.1609 - val_loss: 0.1616\n",
      "Epoch 76/500\n",
      "3750/3750 [==============================] - 2s 568us/sample - loss: 0.1606 - val_loss: 0.1613\n",
      "Epoch 77/500\n",
      "3750/3750 [==============================] - 2s 507us/sample - loss: 0.1603 - val_loss: 0.1610\n",
      "Epoch 78/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1600 - val_loss: 0.1607\n",
      "Epoch 79/500\n",
      "3750/3750 [==============================] - 2s 444us/sample - loss: 0.1597 - val_loss: 0.1604\n",
      "Epoch 80/500\n",
      "3750/3750 [==============================] - 2s 476us/sample - loss: 0.1594 - val_loss: 0.1601\n",
      "Epoch 81/500\n",
      "3750/3750 [==============================] - 2s 521us/sample - loss: 0.1592 - val_loss: 0.1599\n",
      "Epoch 82/500\n",
      "3750/3750 [==============================] - 2s 412us/sample - loss: 0.1589 - val_loss: 0.1596\n",
      "Epoch 83/500\n",
      "3750/3750 [==============================] - 2s 444us/sample - loss: 0.1586 - val_loss: 0.1593\n",
      "Epoch 84/500\n",
      "3750/3750 [==============================] - 2s 442us/sample - loss: 0.1584 - val_loss: 0.1590\n",
      "Epoch 85/500\n",
      "3750/3750 [==============================] - 2s 433us/sample - loss: 0.1581 - val_loss: 0.1588\n",
      "Epoch 86/500\n",
      "3750/3750 [==============================] - 2s 431us/sample - loss: 0.1578 - val_loss: 0.1585\n",
      "Epoch 87/500\n",
      "3750/3750 [==============================] - 2s 437us/sample - loss: 0.1576 - val_loss: 0.1582\n",
      "Epoch 88/500\n",
      "3750/3750 [==============================] - 2s 461us/sample - loss: 0.1573 - val_loss: 0.1580\n",
      "Epoch 89/500\n",
      "3750/3750 [==============================] - 2s 433us/sample - loss: 0.1571 - val_loss: 0.1577\n",
      "Epoch 90/500\n",
      "3750/3750 [==============================] - 2s 456us/sample - loss: 0.1569 - val_loss: 0.1575\n",
      "Epoch 91/500\n",
      "3750/3750 [==============================] - 2s 447us/sample - loss: 0.1566 - val_loss: 0.1572\n",
      "Epoch 92/500\n",
      "3750/3750 [==============================] - 2s 440us/sample - loss: 0.1564 - val_loss: 0.1570\n",
      "Epoch 93/500\n",
      "3750/3750 [==============================] - 2s 519us/sample - loss: 0.1561 - val_loss: 0.1567\n",
      "Epoch 94/500\n",
      "3750/3750 [==============================] - 2s 451us/sample - loss: 0.1559 - val_loss: 0.1565\n",
      "Epoch 95/500\n",
      "3750/3750 [==============================] - 3s 674us/sample - loss: 0.1557 - val_loss: 0.1562\n",
      "Epoch 96/500\n",
      "3750/3750 [==============================] - 3s 679us/sample - loss: 0.1555 - val_loss: 0.1560\n",
      "Epoch 97/500\n",
      "3750/3750 [==============================] - 2s 541us/sample - loss: 0.1553 - val_loss: 0.1558\n",
      "Epoch 98/500\n",
      "3750/3750 [==============================] - 2s 477us/sample - loss: 0.1550 - val_loss: 0.1556\n",
      "Epoch 99/500\n",
      "3750/3750 [==============================] - 2s 579us/sample - loss: 0.1548 - val_loss: 0.1553\n",
      "Epoch 100/500\n",
      "3750/3750 [==============================] - 2s 551us/sample - loss: 0.1546 - val_loss: 0.1551\n",
      "Epoch 101/500\n",
      "3750/3750 [==============================] - 2s 546us/sample - loss: 0.1544 - val_loss: 0.1549\n",
      "Epoch 102/500\n",
      "3750/3750 [==============================] - 3s 782us/sample - loss: 0.1542 - val_loss: 0.1547\n",
      "Epoch 103/500\n",
      "3750/3750 [==============================] - 2s 577us/sample - loss: 0.1541 - val_loss: 0.1545\n",
      "Epoch 104/500\n",
      "3750/3750 [==============================] - 2s 557us/sample - loss: 0.1539 - val_loss: 0.1543\n",
      "Epoch 105/500\n",
      "3750/3750 [==============================] - 2s 478us/sample - loss: 0.1537 - val_loss: 0.1541\n",
      "Epoch 106/500\n",
      "3750/3750 [==============================] - 2s 462us/sample - loss: 0.1535 - val_loss: 0.1539\n",
      "Epoch 107/500\n",
      "3750/3750 [==============================] - 2s 437us/sample - loss: 0.1533 - val_loss: 0.1537\n",
      "Epoch 108/500\n",
      "3750/3750 [==============================] - 2s 511us/sample - loss: 0.1531 - val_loss: 0.1535\n",
      "Epoch 109/500\n",
      "3750/3750 [==============================] - 2s 427us/sample - loss: 0.1529 - val_loss: 0.1533\n",
      "Epoch 110/500\n",
      "3750/3750 [==============================] - 2s 456us/sample - loss: 0.1527 - val_loss: 0.1531\n",
      "Epoch 111/500\n",
      "3750/3750 [==============================] - 2s 481us/sample - loss: 0.1526 - val_loss: 0.1529\n",
      "Epoch 112/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1524 - val_loss: 0.1528\n",
      "Epoch 113/500\n",
      "3750/3750 [==============================] - 2s 453us/sample - loss: 0.1522 - val_loss: 0.1526\n",
      "Epoch 114/500\n",
      "3750/3750 [==============================] - 2s 501us/sample - loss: 0.1521 - val_loss: 0.1524\n",
      "Epoch 115/500\n",
      "3750/3750 [==============================] - 2s 473us/sample - loss: 0.1519 - val_loss: 0.1522\n",
      "Epoch 116/500\n",
      "3750/3750 [==============================] - 2s 480us/sample - loss: 0.1517 - val_loss: 0.1521\n",
      "Epoch 117/500\n",
      "3750/3750 [==============================] - 2s 466us/sample - loss: 0.1516 - val_loss: 0.1519\n",
      "Epoch 118/500\n",
      "3750/3750 [==============================] - 2s 469us/sample - loss: 0.1514 - val_loss: 0.1517\n",
      "Epoch 119/500\n",
      "3750/3750 [==============================] - 2s 431us/sample - loss: 0.1513 - val_loss: 0.1516\n",
      "Epoch 120/500\n",
      "3750/3750 [==============================] - 2s 518us/sample - loss: 0.1511 - val_loss: 0.1514\n",
      "Epoch 121/500\n",
      "3750/3750 [==============================] - 2s 446us/sample - loss: 0.1510 - val_loss: 0.1513\n",
      "Epoch 122/500\n",
      "3750/3750 [==============================] - 2s 444us/sample - loss: 0.1509 - val_loss: 0.1511\n",
      "Epoch 123/500\n",
      "3750/3750 [==============================] - 2s 473us/sample - loss: 0.1507 - val_loss: 0.1509\n",
      "Epoch 124/500\n",
      "3750/3750 [==============================] - 2s 454us/sample - loss: 0.1506 - val_loss: 0.1508\n",
      "Epoch 125/500\n",
      "3750/3750 [==============================] - 2s 417us/sample - loss: 0.1504 - val_loss: 0.1506\n",
      "Epoch 126/500\n",
      "3750/3750 [==============================] - 2s 468us/sample - loss: 0.1503 - val_loss: 0.1505\n",
      "Epoch 127/500\n",
      "3750/3750 [==============================] - 2s 447us/sample - loss: 0.1501 - val_loss: 0.1504\n",
      "Epoch 128/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1500 - val_loss: 0.1502\n",
      "Epoch 129/500\n",
      "3750/3750 [==============================] - 2s 519us/sample - loss: 0.1499 - val_loss: 0.1501\n",
      "Epoch 130/500\n",
      "3750/3750 [==============================] - 2s 477us/sample - loss: 0.1498 - val_loss: 0.1499\n",
      "Epoch 131/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1496 - val_loss: 0.1498\n",
      "Epoch 132/500\n",
      "3750/3750 [==============================] - 2s 602us/sample - loss: 0.1495 - val_loss: 0.1497\n",
      "Epoch 133/500\n",
      "3750/3750 [==============================] - 2s 526us/sample - loss: 0.1494 - val_loss: 0.1496\n",
      "Epoch 134/500\n",
      "3750/3750 [==============================] - 2s 508us/sample - loss: 0.1493 - val_loss: 0.1494\n",
      "Epoch 135/500\n",
      "3750/3750 [==============================] - 3s 848us/sample - loss: 0.1491 - val_loss: 0.1493\n",
      "Epoch 136/500\n",
      "3750/3750 [==============================] - 2s 429us/sample - loss: 0.1490 - val_loss: 0.1492\n",
      "Epoch 137/500\n",
      "3750/3750 [==============================] - 2s 432us/sample - loss: 0.1489 - val_loss: 0.1491\n",
      "Epoch 138/500\n",
      "3750/3750 [==============================] - 2s 427us/sample - loss: 0.1488 - val_loss: 0.1489\n",
      "Epoch 139/500\n",
      "3750/3750 [==============================] - 2s 524us/sample - loss: 0.1487 - val_loss: 0.1488\n",
      "Epoch 140/500\n",
      "3750/3750 [==============================] - 2s 454us/sample - loss: 0.1486 - val_loss: 0.1487\n",
      "Epoch 141/500\n",
      "3750/3750 [==============================] - 2s 443us/sample - loss: 0.1485 - val_loss: 0.1486\n",
      "Epoch 142/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1484 - val_loss: 0.1485\n",
      "Epoch 143/500\n",
      "3750/3750 [==============================] - 2s 434us/sample - loss: 0.1483 - val_loss: 0.1484\n",
      "Epoch 144/500\n",
      "3750/3750 [==============================] - 2s 592us/sample - loss: 0.1482 - val_loss: 0.1483\n",
      "Epoch 145/500\n",
      "3750/3750 [==============================] - 2s 485us/sample - loss: 0.1481 - val_loss: 0.1482\n",
      "Epoch 146/500\n",
      "3750/3750 [==============================] - 2s 442us/sample - loss: 0.1480 - val_loss: 0.1481\n",
      "Epoch 147/500\n",
      "3750/3750 [==============================] - 2s 423us/sample - loss: 0.1479 - val_loss: 0.1480\n",
      "Epoch 148/500\n",
      "3750/3750 [==============================] - 2s 450us/sample - loss: 0.1478 - val_loss: 0.1479\n",
      "Epoch 149/500\n",
      "3750/3750 [==============================] - 2s 442us/sample - loss: 0.1477 - val_loss: 0.1478\n",
      "Epoch 150/500\n",
      "3750/3750 [==============================] - 2s 444us/sample - loss: 0.1476 - val_loss: 0.1477\n",
      "Epoch 151/500\n",
      "3750/3750 [==============================] - 2s 444us/sample - loss: 0.1475 - val_loss: 0.1476\n",
      "Epoch 152/500\n",
      "3750/3750 [==============================] - 2s 433us/sample - loss: 0.1474 - val_loss: 0.1475\n",
      "Epoch 153/500\n",
      "3750/3750 [==============================] - 2s 481us/sample - loss: 0.1473 - val_loss: 0.1474\n",
      "Epoch 154/500\n",
      "3750/3750 [==============================] - 2s 446us/sample - loss: 0.1472 - val_loss: 0.1473\n",
      "Epoch 155/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1471 - val_loss: 0.1472\n",
      "Epoch 156/500\n",
      "3750/3750 [==============================] - 2s 443us/sample - loss: 0.1471 - val_loss: 0.1471\n",
      "Epoch 157/500\n",
      "3750/3750 [==============================] - 2s 455us/sample - loss: 0.1470 - val_loss: 0.1470\n",
      "Epoch 158/500\n",
      "3750/3750 [==============================] - 2s 483us/sample - loss: 0.1469 - val_loss: 0.1469\n",
      "Epoch 159/500\n",
      "3750/3750 [==============================] - 2s 480us/sample - loss: 0.1468 - val_loss: 0.1469\n",
      "Epoch 160/500\n",
      "3750/3750 [==============================] - 2s 425us/sample - loss: 0.1467 - val_loss: 0.1468\n",
      "Epoch 161/500\n",
      "3750/3750 [==============================] - 2s 453us/sample - loss: 0.1466 - val_loss: 0.1467\n",
      "Epoch 162/500\n",
      "3750/3750 [==============================] - 2s 445us/sample - loss: 0.1466 - val_loss: 0.1466\n",
      "Epoch 163/500\n",
      "3750/3750 [==============================] - 2s 439us/sample - loss: 0.1465 - val_loss: 0.1465\n",
      "Epoch 164/500\n",
      "3750/3750 [==============================] - 2s 469us/sample - loss: 0.1464 - val_loss: 0.1465\n",
      "Epoch 165/500\n",
      "3750/3750 [==============================] - 2s 443us/sample - loss: 0.1464 - val_loss: 0.1464\n",
      "Epoch 166/500\n",
      "3750/3750 [==============================] - 2s 489us/sample - loss: 0.1463 - val_loss: 0.1463\n",
      "Epoch 167/500\n",
      "3750/3750 [==============================] - 2s 457us/sample - loss: 0.1462 - val_loss: 0.1463\n",
      "Epoch 168/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1461 - val_loss: 0.1462\n",
      "Epoch 169/500\n",
      "3750/3750 [==============================] - 3s 848us/sample - loss: 0.1461 - val_loss: 0.1461\n",
      "Epoch 170/500\n",
      "3750/3750 [==============================] - 2s 616us/sample - loss: 0.1460 - val_loss: 0.1460\n",
      "Epoch 171/500\n",
      "3750/3750 [==============================] - 2s 435us/sample - loss: 0.1459 - val_loss: 0.1460\n",
      "Epoch 172/500\n",
      "3750/3750 [==============================] - 2s 435us/sample - loss: 0.1458 - val_loss: 0.1459\n",
      "Epoch 173/500\n",
      "3750/3750 [==============================] - 2s 440us/sample - loss: 0.1458 - val_loss: 0.1458\n",
      "Epoch 174/500\n",
      "3750/3750 [==============================] - 2s 457us/sample - loss: 0.1457 - val_loss: 0.1458\n",
      "Epoch 175/500\n",
      "3750/3750 [==============================] - 2s 451us/sample - loss: 0.1456 - val_loss: 0.1457\n",
      "Epoch 176/500\n",
      "3750/3750 [==============================] - 2s 510us/sample - loss: 0.1456 - val_loss: 0.1456\n",
      "Epoch 177/500\n",
      "3750/3750 [==============================] - 2s 432us/sample - loss: 0.1455 - val_loss: 0.1456\n",
      "Epoch 178/500\n",
      "3750/3750 [==============================] - 2s 459us/sample - loss: 0.1455 - val_loss: 0.1455\n",
      "Epoch 179/500\n",
      "3750/3750 [==============================] - 2s 465us/sample - loss: 0.1454 - val_loss: 0.1454\n",
      "Epoch 180/500\n",
      "3750/3750 [==============================] - 2s 461us/sample - loss: 0.1453 - val_loss: 0.1454\n",
      "Epoch 181/500\n",
      "3750/3750 [==============================] - 2s 495us/sample - loss: 0.1453 - val_loss: 0.1453\n",
      "Epoch 182/500\n",
      "3750/3750 [==============================] - 2s 434us/sample - loss: 0.1452 - val_loss: 0.1453\n",
      "Epoch 183/500\n",
      "3750/3750 [==============================] - 2s 425us/sample - loss: 0.1451 - val_loss: 0.1452\n",
      "Epoch 184/500\n",
      "3750/3750 [==============================] - 2s 452us/sample - loss: 0.1451 - val_loss: 0.1452\n",
      "Epoch 185/500\n",
      "3750/3750 [==============================] - 2s 456us/sample - loss: 0.1450 - val_loss: 0.1451\n",
      "Epoch 186/500\n",
      "3750/3750 [==============================] - 2s 428us/sample - loss: 0.1450 - val_loss: 0.1451\n",
      "Epoch 187/500\n",
      "3750/3750 [==============================] - 2s 438us/sample - loss: 0.1449 - val_loss: 0.1450\n",
      "Epoch 188/500\n",
      "3750/3750 [==============================] - 2s 449us/sample - loss: 0.1448 - val_loss: 0.1450\n",
      "Epoch 189/500\n",
      "3750/3750 [==============================] - 2s 465us/sample - loss: 0.1448 - val_loss: 0.1449\n",
      "Epoch 190/500\n",
      "3750/3750 [==============================] - 2s 497us/sample - loss: 0.1447 - val_loss: 0.1448\n",
      "Epoch 191/500\n",
      "3750/3750 [==============================] - 2s 470us/sample - loss: 0.1447 - val_loss: 0.1448\n",
      "Epoch 192/500\n",
      "3750/3750 [==============================] - 2s 445us/sample - loss: 0.1446 - val_loss: 0.1448\n",
      "Epoch 193/500\n",
      "3750/3750 [==============================] - 2s 451us/sample - loss: 0.1446 - val_loss: 0.1447\n",
      "Epoch 194/500\n",
      "3750/3750 [==============================] - 2s 480us/sample - loss: 0.1445 - val_loss: 0.1447\n",
      "Epoch 195/500\n",
      "3750/3750 [==============================] - 2s 454us/sample - loss: 0.1445 - val_loss: 0.1446\n",
      "Epoch 196/500\n",
      "3750/3750 [==============================] - 2s 494us/sample - loss: 0.1444 - val_loss: 0.1446\n",
      "Epoch 197/500\n",
      "3750/3750 [==============================] - 2s 512us/sample - loss: 0.1444 - val_loss: 0.1445\n",
      "Epoch 198/500\n",
      "3750/3750 [==============================] - 2s 452us/sample - loss: 0.1443 - val_loss: 0.1445\n",
      "Epoch 199/500\n",
      "3750/3750 [==============================] - 2s 471us/sample - loss: 0.1442 - val_loss: 0.1444\n",
      "Epoch 200/500\n",
      "3750/3750 [==============================] - 2s 545us/sample - loss: 0.1442 - val_loss: 0.1444\n",
      "Epoch 201/500\n",
      "3750/3750 [==============================] - 2s 450us/sample - loss: 0.1442 - val_loss: 0.1443\n",
      "Epoch 202/500\n",
      "3750/3750 [==============================] - 2s 443us/sample - loss: 0.1441 - val_loss: 0.1443\n",
      "Epoch 203/500\n",
      "3750/3750 [==============================] - 4s 975us/sample - loss: 0.1440 - val_loss: 0.1442\n",
      "Epoch 204/500\n",
      "3750/3750 [==============================] - 3s 681us/sample - loss: 0.1440 - val_loss: 0.1442\n",
      "Epoch 205/500\n",
      "3750/3750 [==============================] - 2s 495us/sample - loss: 0.1440 - val_loss: 0.1442\n",
      "Epoch 206/500\n",
      "3750/3750 [==============================] - 2s 488us/sample - loss: 0.1439 - val_loss: 0.1441\n",
      "Epoch 207/500\n",
      "3750/3750 [==============================] - 2s 465us/sample - loss: 0.1439 - val_loss: 0.1441\n",
      "Epoch 208/500\n",
      "3750/3750 [==============================] - 2s 442us/sample - loss: 0.1438 - val_loss: 0.1440\n",
      "Epoch 209/500\n",
      "3750/3750 [==============================] - 2s 428us/sample - loss: 0.1438 - val_loss: 0.1440\n",
      "Epoch 210/500\n",
      "3750/3750 [==============================] - 2s 464us/sample - loss: 0.1437 - val_loss: 0.1439\n",
      "Epoch 211/500\n",
      "3750/3750 [==============================] - 2s 472us/sample - loss: 0.1437 - val_loss: 0.1439\n",
      "Epoch 212/500\n",
      "3750/3750 [==============================] - 2s 431us/sample - loss: 0.1437 - val_loss: 0.1439\n",
      "Epoch 213/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1436 - val_loss: 0.1438\n",
      "Epoch 214/500\n",
      "3750/3750 [==============================] - 2s 427us/sample - loss: 0.1436 - val_loss: 0.1438\n",
      "Epoch 215/500\n",
      "3750/3750 [==============================] - 2s 446us/sample - loss: 0.1435 - val_loss: 0.1437\n",
      "Epoch 216/500\n",
      "3750/3750 [==============================] - 2s 489us/sample - loss: 0.1434 - val_loss: 0.1437\n",
      "Epoch 217/500\n",
      "3750/3750 [==============================] - 2s 433us/sample - loss: 0.1434 - val_loss: 0.1437\n",
      "Epoch 218/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1434 - val_loss: 0.1436\n",
      "Epoch 219/500\n",
      "3750/3750 [==============================] - 2s 439us/sample - loss: 0.1433 - val_loss: 0.1436\n",
      "Epoch 220/500\n",
      "3750/3750 [==============================] - 2s 457us/sample - loss: 0.1433 - val_loss: 0.1436\n",
      "Epoch 221/500\n",
      "3750/3750 [==============================] - 2s 579us/sample - loss: 0.1433 - val_loss: 0.1435\n",
      "Epoch 222/500\n",
      "3750/3750 [==============================] - 2s 456us/sample - loss: 0.1432 - val_loss: 0.1435\n",
      "Epoch 223/500\n",
      "3750/3750 [==============================] - 2s 461us/sample - loss: 0.1432 - val_loss: 0.1435\n",
      "Epoch 224/500\n",
      "3750/3750 [==============================] - 2s 453us/sample - loss: 0.1432 - val_loss: 0.1434\n",
      "Epoch 225/500\n",
      "3750/3750 [==============================] - 2s 449us/sample - loss: 0.1431 - val_loss: 0.1434\n",
      "Epoch 226/500\n",
      "3750/3750 [==============================] - 2s 489us/sample - loss: 0.1431 - val_loss: 0.1433\n",
      "Epoch 227/500\n",
      "3750/3750 [==============================] - 2s 565us/sample - loss: 0.1430 - val_loss: 0.1433\n",
      "Epoch 228/500\n",
      "3750/3750 [==============================] - 2s 428us/sample - loss: 0.1430 - val_loss: 0.1433\n",
      "Epoch 229/500\n",
      "3750/3750 [==============================] - 2s 592us/sample - loss: 0.1429 - val_loss: 0.1432\n",
      "Epoch 230/500\n",
      "3750/3750 [==============================] - 2s 429us/sample - loss: 0.1429 - val_loss: 0.1432\n",
      "Epoch 231/500\n",
      "3750/3750 [==============================] - 2s 447us/sample - loss: 0.1428 - val_loss: 0.1432\n",
      "Epoch 232/500\n",
      "3750/3750 [==============================] - 2s 463us/sample - loss: 0.1428 - val_loss: 0.1431\n",
      "Epoch 233/500\n",
      "3750/3750 [==============================] - 2s 442us/sample - loss: 0.1428 - val_loss: 0.1431\n",
      "Epoch 234/500\n",
      "3750/3750 [==============================] - 2s 438us/sample - loss: 0.1427 - val_loss: 0.1431\n",
      "Epoch 235/500\n",
      "3750/3750 [==============================] - 2s 531us/sample - loss: 0.1427 - val_loss: 0.1430\n",
      "Epoch 236/500\n",
      "3750/3750 [==============================] - 2s 578us/sample - loss: 0.1427 - val_loss: 0.1430\n",
      "Epoch 237/500\n",
      "3750/3750 [==============================] - 2s 402us/sample - loss: 0.1426 - val_loss: 0.1430\n",
      "Epoch 238/500\n",
      "3750/3750 [==============================] - 2s 467us/sample - loss: 0.1426 - val_loss: 0.1430\n",
      "Epoch 239/500\n",
      "3750/3750 [==============================] - 2s 477us/sample - loss: 0.1425 - val_loss: 0.1429\n",
      "Epoch 240/500\n",
      "3750/3750 [==============================] - 2s 469us/sample - loss: 0.1425 - val_loss: 0.1429\n",
      "Epoch 241/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1425 - val_loss: 0.1429\n",
      "Epoch 242/500\n",
      "3750/3750 [==============================] - 2s 447us/sample - loss: 0.1424 - val_loss: 0.1428\n",
      "Epoch 243/500\n",
      "3750/3750 [==============================] - 2s 437us/sample - loss: 0.1424 - val_loss: 0.1428\n",
      "Epoch 244/500\n",
      "3750/3750 [==============================] - 2s 444us/sample - loss: 0.1424 - val_loss: 0.1428\n",
      "Epoch 245/500\n",
      "3750/3750 [==============================] - 2s 476us/sample - loss: 0.1423 - val_loss: 0.1428\n",
      "Epoch 246/500\n",
      "3750/3750 [==============================] - 2s 425us/sample - loss: 0.1423 - val_loss: 0.1427\n",
      "Epoch 247/500\n",
      "3750/3750 [==============================] - 2s 449us/sample - loss: 0.1423 - val_loss: 0.1427\n",
      "Epoch 248/500\n",
      "3750/3750 [==============================] - 2s 429us/sample - loss: 0.1422 - val_loss: 0.1427\n",
      "Epoch 249/500\n",
      "3750/3750 [==============================] - 2s 439us/sample - loss: 0.1422 - val_loss: 0.1427\n",
      "Epoch 250/500\n",
      "3750/3750 [==============================] - 2s 438us/sample - loss: 0.1422 - val_loss: 0.1426\n",
      "Epoch 251/500\n",
      "3750/3750 [==============================] - 2s 486us/sample - loss: 0.1421 - val_loss: 0.1426\n",
      "Epoch 252/500\n",
      "3750/3750 [==============================] - 2s 438us/sample - loss: 0.1421 - val_loss: 0.1426\n",
      "Epoch 253/500\n",
      "3750/3750 [==============================] - 2s 423us/sample - loss: 0.1421 - val_loss: 0.1425\n",
      "Epoch 254/500\n",
      "3750/3750 [==============================] - 2s 430us/sample - loss: 0.1420 - val_loss: 0.1425\n",
      "Epoch 255/500\n",
      "3750/3750 [==============================] - 2s 459us/sample - loss: 0.1420 - val_loss: 0.1425\n",
      "Epoch 256/500\n",
      "3750/3750 [==============================] - 2s 439us/sample - loss: 0.1420 - val_loss: 0.1425\n",
      "Epoch 257/500\n",
      "3750/3750 [==============================] - 2s 499us/sample - loss: 0.1419 - val_loss: 0.1425\n",
      "Epoch 258/500\n",
      "3750/3750 [==============================] - 2s 456us/sample - loss: 0.1419 - val_loss: 0.1424\n",
      "Epoch 259/500\n",
      "3750/3750 [==============================] - 2s 436us/sample - loss: 0.1419 - val_loss: 0.1424\n",
      "Epoch 260/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1418 - val_loss: 0.1424\n",
      "Epoch 261/500\n",
      "3750/3750 [==============================] - 2s 526us/sample - loss: 0.1418 - val_loss: 0.1424\n",
      "Epoch 262/500\n",
      "3750/3750 [==============================] - 2s 612us/sample - loss: 0.1418 - val_loss: 0.1423\n",
      "Epoch 263/500\n",
      "3750/3750 [==============================] - 2s 455us/sample - loss: 0.1418 - val_loss: 0.1423\n",
      "Epoch 264/500\n",
      "3750/3750 [==============================] - 2s 460us/sample - loss: 0.1417 - val_loss: 0.1423\n",
      "Epoch 265/500\n",
      "3750/3750 [==============================] - 2s 435us/sample - loss: 0.1417 - val_loss: 0.1423\n",
      "Epoch 266/500\n",
      "3750/3750 [==============================] - 2s 431us/sample - loss: 0.1417 - val_loss: 0.1422\n",
      "Epoch 267/500\n",
      "3750/3750 [==============================] - 2s 540us/sample - loss: 0.1416 - val_loss: 0.1422\n",
      "Epoch 268/500\n",
      "3750/3750 [==============================] - 2s 506us/sample - loss: 0.1416 - val_loss: 0.1422\n",
      "Epoch 269/500\n",
      "3750/3750 [==============================] - 2s 450us/sample - loss: 0.1416 - val_loss: 0.1422\n",
      "Epoch 270/500\n",
      "3750/3750 [==============================] - 3s 746us/sample - loss: 0.1415 - val_loss: 0.1422\n",
      "Epoch 271/500\n",
      "3750/3750 [==============================] - 2s 562us/sample - loss: 0.1415 - val_loss: 0.1421\n",
      "Epoch 272/500\n",
      "3750/3750 [==============================] - 2s 607us/sample - loss: 0.1415 - val_loss: 0.1421\n",
      "Epoch 273/500\n",
      "3750/3750 [==============================] - 2s 492us/sample - loss: 0.1415 - val_loss: 0.1421\n",
      "Epoch 274/500\n",
      "3750/3750 [==============================] - 2s 431us/sample - loss: 0.1414 - val_loss: 0.1421\n",
      "Epoch 275/500\n",
      "3750/3750 [==============================] - 2s 457us/sample - loss: 0.1414 - val_loss: 0.1421\n",
      "Epoch 276/500\n",
      "3750/3750 [==============================] - 2s 422us/sample - loss: 0.1414 - val_loss: 0.1420\n",
      "Epoch 277/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1413 - val_loss: 0.1420\n",
      "Epoch 278/500\n",
      "3750/3750 [==============================] - 2s 467us/sample - loss: 0.1413 - val_loss: 0.1420\n",
      "Epoch 279/500\n",
      "3750/3750 [==============================] - 2s 531us/sample - loss: 0.1413 - val_loss: 0.1420\n",
      "Epoch 280/500\n",
      "3750/3750 [==============================] - 2s 542us/sample - loss: 0.1413 - val_loss: 0.1419\n",
      "Epoch 281/500\n",
      "3750/3750 [==============================] - 2s 426us/sample - loss: 0.1412 - val_loss: 0.1419\n",
      "Epoch 282/500\n",
      "3750/3750 [==============================] - 2s 477us/sample - loss: 0.1412 - val_loss: 0.1419\n",
      "Epoch 283/500\n",
      "3750/3750 [==============================] - 2s 435us/sample - loss: 0.1412 - val_loss: 0.1419\n",
      "Epoch 284/500\n",
      "3750/3750 [==============================] - 2s 435us/sample - loss: 0.1412 - val_loss: 0.1419\n",
      "Epoch 285/500\n",
      "3750/3750 [==============================] - 2s 459us/sample - loss: 0.1411 - val_loss: 0.1419\n",
      "Epoch 286/500\n",
      "3750/3750 [==============================] - 2s 534us/sample - loss: 0.1411 - val_loss: 0.1418\n",
      "Epoch 287/500\n",
      "3750/3750 [==============================] - 2s 452us/sample - loss: 0.1411 - val_loss: 0.1418\n",
      "Epoch 288/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1411 - val_loss: 0.1418\n",
      "Epoch 289/500\n",
      "3750/3750 [==============================] - 2s 457us/sample - loss: 0.1410 - val_loss: 0.1418\n",
      "Epoch 290/500\n",
      "3750/3750 [==============================] - 2s 433us/sample - loss: 0.1410 - val_loss: 0.1418\n",
      "Epoch 291/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1410 - val_loss: 0.1418\n",
      "Epoch 292/500\n",
      "3750/3750 [==============================] - 2s 464us/sample - loss: 0.1410 - val_loss: 0.1417\n",
      "Epoch 293/500\n",
      "3750/3750 [==============================] - 2s 426us/sample - loss: 0.1409 - val_loss: 0.1417\n",
      "Epoch 294/500\n",
      "3750/3750 [==============================] - 2s 419us/sample - loss: 0.1409 - val_loss: 0.1417\n",
      "Epoch 295/500\n",
      "3750/3750 [==============================] - 2s 442us/sample - loss: 0.1409 - val_loss: 0.1417\n",
      "Epoch 296/500\n",
      "3750/3750 [==============================] - 2s 484us/sample - loss: 0.1408 - val_loss: 0.1417\n",
      "Epoch 297/500\n",
      "3750/3750 [==============================] - 2s 444us/sample - loss: 0.1408 - val_loss: 0.1417\n",
      "Epoch 298/500\n",
      "3750/3750 [==============================] - 2s 433us/sample - loss: 0.1408 - val_loss: 0.1417\n",
      "Epoch 299/500\n",
      "3750/3750 [==============================] - 2s 454us/sample - loss: 0.1408 - val_loss: 0.1416\n",
      "Epoch 300/500\n",
      "3750/3750 [==============================] - 2s 450us/sample - loss: 0.1408 - val_loss: 0.1416\n",
      "Epoch 301/500\n",
      "3750/3750 [==============================] - 2s 416us/sample - loss: 0.1407 - val_loss: 0.1416\n",
      "Epoch 302/500\n",
      "3750/3750 [==============================] - 2s 438us/sample - loss: 0.1407 - val_loss: 0.1416\n",
      "Epoch 303/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1407 - val_loss: 0.1416\n",
      "Epoch 304/500\n",
      "3750/3750 [==============================] - 3s 754us/sample - loss: 0.1407 - val_loss: 0.1415\n",
      "Epoch 305/500\n",
      "3750/3750 [==============================] - 2s 530us/sample - loss: 0.1407 - val_loss: 0.1416\n",
      "Epoch 306/500\n",
      "3750/3750 [==============================] - 2s 463us/sample - loss: 0.1406 - val_loss: 0.1415\n",
      "Epoch 307/500\n",
      "3750/3750 [==============================] - 2s 433us/sample - loss: 0.1406 - val_loss: 0.1415\n",
      "Epoch 308/500\n",
      "3750/3750 [==============================] - 2s 434us/sample - loss: 0.1406 - val_loss: 0.1415\n",
      "Epoch 309/500\n",
      "3750/3750 [==============================] - 2s 461us/sample - loss: 0.1405 - val_loss: 0.1415\n",
      "Epoch 310/500\n",
      "3750/3750 [==============================] - 2s 609us/sample - loss: 0.1405 - val_loss: 0.1414\n",
      "Epoch 311/500\n",
      "3750/3750 [==============================] - 2s 429us/sample - loss: 0.1405 - val_loss: 0.1415\n",
      "Epoch 312/500\n",
      "3750/3750 [==============================] - 2s 428us/sample - loss: 0.1405 - val_loss: 0.1415\n",
      "Epoch 313/500\n",
      "3750/3750 [==============================] - 2s 459us/sample - loss: 0.1404 - val_loss: 0.1414\n",
      "Epoch 314/500\n",
      "3750/3750 [==============================] - 2s 442us/sample - loss: 0.1405 - val_loss: 0.1414\n",
      "Epoch 315/500\n",
      "3750/3750 [==============================] - 2s 443us/sample - loss: 0.1404 - val_loss: 0.1414\n",
      "Epoch 316/500\n",
      "3750/3750 [==============================] - 2s 460us/sample - loss: 0.1404 - val_loss: 0.1414\n",
      "Epoch 317/500\n",
      "3750/3750 [==============================] - 2s 425us/sample - loss: 0.1404 - val_loss: 0.1414\n",
      "Epoch 318/500\n",
      "3750/3750 [==============================] - 2s 449us/sample - loss: 0.1403 - val_loss: 0.1414\n",
      "Epoch 319/500\n",
      "3750/3750 [==============================] - 2s 449us/sample - loss: 0.1403 - val_loss: 0.1413\n",
      "Epoch 320/500\n",
      "3750/3750 [==============================] - 2s 443us/sample - loss: 0.1403 - val_loss: 0.1414\n",
      "Epoch 321/500\n",
      "3750/3750 [==============================] - 2s 427us/sample - loss: 0.1403 - val_loss: 0.1413\n",
      "Epoch 322/500\n",
      "3750/3750 [==============================] - 2s 435us/sample - loss: 0.1402 - val_loss: 0.1413\n",
      "Epoch 323/500\n",
      "3750/3750 [==============================] - 2s 496us/sample - loss: 0.1402 - val_loss: 0.1413\n",
      "Epoch 324/500\n",
      "3750/3750 [==============================] - 2s 576us/sample - loss: 0.1402 - val_loss: 0.1413\n",
      "Epoch 325/500\n",
      "3750/3750 [==============================] - 2s 476us/sample - loss: 0.1402 - val_loss: 0.1413\n",
      "Epoch 326/500\n",
      "3750/3750 [==============================] - 4s 1ms/sample - loss: 0.1402 - val_loss: 0.1413\n",
      "Epoch 327/500\n",
      "3750/3750 [==============================] - 3s 711us/sample - loss: 0.1401 - val_loss: 0.1412\n",
      "Epoch 328/500\n",
      "3750/3750 [==============================] - 3s 740us/sample - loss: 0.1401 - val_loss: 0.1412\n",
      "Epoch 329/500\n",
      "3750/3750 [==============================] - 2s 490us/sample - loss: 0.1401 - val_loss: 0.1412\n",
      "Epoch 330/500\n",
      "3750/3750 [==============================] - 2s 541us/sample - loss: 0.1401 - val_loss: 0.1412\n",
      "Epoch 331/500\n",
      "3750/3750 [==============================] - 2s 605us/sample - loss: 0.1401 - val_loss: 0.1412\n",
      "Epoch 332/500\n",
      "3750/3750 [==============================] - 2s 569us/sample - loss: 0.1400 - val_loss: 0.1412\n",
      "Epoch 333/500\n",
      "3750/3750 [==============================] - 2s 424us/sample - loss: 0.1400 - val_loss: 0.1412\n",
      "Epoch 334/500\n",
      "3750/3750 [==============================] - 3s 701us/sample - loss: 0.1400 - val_loss: 0.1412\n",
      "Epoch 335/500\n",
      "3750/3750 [==============================] - 2s 463us/sample - loss: 0.1400 - val_loss: 0.1412\n",
      "Epoch 336/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1400 - val_loss: 0.1412\n",
      "Epoch 337/500\n",
      "3750/3750 [==============================] - 2s 431us/sample - loss: 0.1399 - val_loss: 0.1411\n",
      "Epoch 338/500\n",
      "3750/3750 [==============================] - 2s 445us/sample - loss: 0.1399 - val_loss: 0.1411\n",
      "Epoch 339/500\n",
      "3750/3750 [==============================] - 2s 431us/sample - loss: 0.1399 - val_loss: 0.1411\n",
      "Epoch 340/500\n",
      "3750/3750 [==============================] - 2s 441us/sample - loss: 0.1399 - val_loss: 0.1411\n",
      "Epoch 341/500\n",
      "3750/3750 [==============================] - 2s 442us/sample - loss: 0.1398 - val_loss: 0.1411\n",
      "Epoch 342/500\n",
      "3750/3750 [==============================] - 2s 438us/sample - loss: 0.1398 - val_loss: 0.1411\n",
      "Epoch 343/500\n",
      "3750/3750 [==============================] - 2s 445us/sample - loss: 0.1398 - val_loss: 0.1410\n",
      "Epoch 344/500\n",
      "3750/3750 [==============================] - 2s 543us/sample - loss: 0.1398 - val_loss: 0.1411\n",
      "Epoch 345/500\n",
      "3750/3750 [==============================] - 3s 871us/sample - loss: 0.1398 - val_loss: 0.1410\n",
      "Epoch 346/500\n",
      "3750/3750 [==============================] - 4s 998us/sample - loss: 0.1398 - val_loss: 0.1410\n",
      "Epoch 347/500\n",
      "3750/3750 [==============================] - 3s 718us/sample - loss: 0.1397 - val_loss: 0.1410\n",
      "Epoch 348/500\n",
      "3750/3750 [==============================] - 2s 532us/sample - loss: 0.1397 - val_loss: 0.1410\n",
      "Epoch 349/500\n",
      "3750/3750 [==============================] - 2s 536us/sample - loss: 0.1397 - val_loss: 0.1410\n",
      "Epoch 350/500\n",
      "3750/3750 [==============================] - 4s 976us/sample - loss: 0.1397 - val_loss: 0.1410\n",
      "Epoch 351/500\n",
      "3750/3750 [==============================] - 2s 530us/sample - loss: 0.1397 - val_loss: 0.1410\n",
      "Epoch 352/500\n",
      "3750/3750 [==============================] - 8s 2ms/sample - loss: 0.1397 - val_loss: 0.1409\n",
      "Epoch 353/500\n",
      "3750/3750 [==============================] - 5s 1ms/sample - loss: 0.1397 - val_loss: 0.1410\n",
      "Epoch 354/500\n",
      "3750/3750 [==============================] - 3s 813us/sample - loss: 0.1396 - val_loss: 0.1409\n",
      "Epoch 355/500\n",
      "3750/3750 [==============================] - 2s 639us/sample - loss: 0.1396 - val_loss: 0.1409\n",
      "Epoch 356/500\n",
      "3750/3750 [==============================] - 2s 633us/sample - loss: 0.1396 - val_loss: 0.1409\n",
      "Epoch 357/500\n",
      "3750/3750 [==============================] - 2s 619us/sample - loss: 0.1395 - val_loss: 0.1409\n",
      "Epoch 358/500\n",
      "3750/3750 [==============================] - 2s 572us/sample - loss: 0.1395 - val_loss: 0.1409\n",
      "Epoch 359/500\n",
      "3750/3750 [==============================] - 3s 721us/sample - loss: 0.1395 - val_loss: 0.1409\n",
      "Epoch 360/500\n",
      "3750/3750 [==============================] - 2s 499us/sample - loss: 0.1395 - val_loss: 0.1409\n",
      "Epoch 361/500\n",
      "1428/3750 [==========>...................] - ETA: 0s - loss: 0.1372WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2900e9ce290e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/cfl/cfl/experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, prev_results)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Beginning {block.get_name()} training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/cfl/cfl/cond_density_estimation/condDensityEstimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, prev_results)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mTODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\"\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/cfl/cfl/cond_density_estimation/condExpBase.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, prev_results)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             )\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda2/envs/cfl_env/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_results = my_exp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `train()` on the CFL `Experiment` results in a dictionary of dictionaries. The keys of the first dictionary correspond to the `Blocks` in the CFL pipeline. In this case (and in all cases at the moment), they are `CDE` and `Clusterer`. The keys of the inner dictionaries are the results returned by each `Block`. Below, we access the cause macrostate labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 1 1 3 3 0 2 1 3 3 1 3 0 3 1 2 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Below, we print the first few `x_lbls`. \n",
    "# We can see that there are 4 classes in the data, and \n",
    "# that they are represented by the numbers `0` through `3`. Each of these labels tells us the \n",
    "# macrovariable to which the corresponding visual bars image was assigned. \n",
    "\n",
    "cause_cluster_results = train_results['CauseClusterer']\n",
    "print(cause_cluster_results['x_lbls'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optional: Predicting on a New Dataset\n",
    "\n",
    "To predict on different data using the same, already trained CFL pipeline, we just create a second data set, and call the predict method on that new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Experiment prediction.\n",
      "Beginning CDE prediction...\n",
      "CDE prediction complete.\n",
      "Beginning CauseClusterer prediction...\n",
      "CauseClusterer prediction complete.\n",
      "Beginning EffectClusterer prediction...\n",
      "EffectClusterer prediction complete.\n",
      "Prediction complete.\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "im_shape = (10, 10)\n",
    "noise_lvl= 0.03\n",
    "random_state = 180 \n",
    "\n",
    "# make second dataset for prediction\n",
    "vb_data = vbd.VisualBarsData(n_samples=n_samples, im_shape = im_shape, noise_lvl=noise_lvl, set_random_seed=random_state)\n",
    "\n",
    "# retrieve the images and the target \n",
    "X_new = vb_data.getImages()\n",
    "Y_new = vb_data.getTarget()\n",
    "\n",
    "#reformat x, y into the right shape for the neural net \n",
    "X_new_CNN = np.expand_dims(X_new, -1) \n",
    "Y_new_CNN = np.expand_dims(Y_new, -1)\n",
    "\n",
    "# put X, Y into a new Dataset object\n",
    "# and register the new dataset with the Experiment\n",
    "my_exp.add_dataset(X=X_new_CNN, Y=Y_new_CNN, dataset_name='predict_data')  \n",
    "\n",
    "# predict! \n",
    "results_new = my_exp.predict('predict_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 2, 0, 3, 1, 3, 1, 2, 1, 1, 0, 1, 3, 0, 3, 0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at some of the results for the new data set \n",
    "cluster_results_new = results_new['CauseClusterer']\n",
    "cluster_results_new['x_lbls'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results \n",
    "\n",
    "We can view some images with their predicted label using the `viewImagesandLabels()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x1440 with 40 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import visual_bars.visual_bars_vis as vis\n",
    "\n",
    "vis.viewImagesAndLabels(X_new, im_shape=im_shape, n_examples=10, x_lbls=cluster_results_new['x_lbls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, CFL has done a pretty good job of recovering the observational partitions (with some errors). For the most part, the images with a horizontal bar, a vertical bar, both, or neither, have been separated into distinct classes. Try this experiment again with a different CDE or a different sample size, and see how your results differ! "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9f90a7866deec4ec91106178f5b99f4479e17c39f9cdadf9218e367e68d50a4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('cfl_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
